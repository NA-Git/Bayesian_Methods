{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from multiprocessing import Pool\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import arviz as az\n",
    "import pymc3 as pm\n",
    "from theano import shared\n",
    "from sklearn import preprocessing\n",
    "import shap\n",
    "import bambi as bmb\n",
    "import formulae\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from fitter import Fitter, get_common_distributions, get_distributions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/matt/Documents/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pandas dataframes will have string, int, and float columns. The following\n",
    "three sections will look for columns that need to be fixed or dropped altogether"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these drop column snippets are not used often here,\n",
    "# but have been useful, especially with large datasets\n",
    "\n",
    "df = df.drop(['Program Name', 'Retailers', 'Tactic', 'Vendor', 'Tactic Start Date', 'Tactic End Date', 'Brand'], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()\n",
    "print(df_cat.nunique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.RMN.replace(('Yes', 'No'), (1, 0), inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# target = 'Total Sales'\n",
    "# CAT_FEATURES = ['Tactic Category']\n",
    "#\n",
    "# df_cat = df.dropna(subset=['Total Sales'])\n",
    "# df_cat = df_cat[['Total Sales', 'Tactic Category']]\n",
    "# df_train, df_test = train_test_split(df_cat, test_size=0.3)\n",
    "# X_train, y_train = df_train.drop(target, axis=1), df_train[target]\n",
    "# X_test, y_test = df_test.drop(target, axis=1), df_test[target]\n",
    "#\n",
    "# catboost_model = CatBoostRegressor(n_estimators=200,\n",
    "#                                    loss_function = 'RMSE',\n",
    "#                                    eval_metric = 'RMSE',\n",
    "#                                    cat_features = CAT_FEATURES, one_hot_max_size=20)\n",
    "# catboost_model.fit(X_train, y_train, cat_features = CAT_FEATURES,\n",
    "#                    eval_set = (X_test, y_test),\n",
    "#                    use_best_model = True,\n",
    "#                    plot = True)\n",
    "# shap_values = catboost_model.get_feature_importance(Pool(\n",
    "#     X_train,\n",
    "#     label = y_train,\n",
    "#     cat_features = CAT_FEATURES\n",
    "# ),type = \"ShapValues\")\n",
    "# shap_values = shap_values[:,:-1]\n",
    "# shap.summary_plot(shap_values, X_train, max_display=50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the below section we address the large number of missing values and also\n",
    "the columns consisting entirely of zeroes, and drop them accordingly"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "        ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "         '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "         'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "        ['ClientId', 'Program Id', 'TacticId', 'CategoryId',\n",
    "         'BrandId', 'Nielsen_Week_Year', 'VendorId'], axis=1)\n",
    "df_num = df.select_dtypes(exclude='object')\n",
    "df_num.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following heat maps are obviously important for understanding relationships,\n",
    "but more importantly their dataframes provide the ability to fill df.colnames\n",
    "that will be key to making a decision on what variables to explore for feature importance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr = df_num.corr(method=\"spearman\").round(2)\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "f, ax = plt.subplots(figsize=(18, 18))\n",
    "cmap = sns.diverging_palette(250, 1, as_cmap=True)\n",
    "sns.heatmap(corr, annot=True, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "corr.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "viz = plot_corr_heatmap(df_num, cmap='CMRmap', value_fontsize=14,\n",
    "                        label_fontsize=14, figsize=(16, 16))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we use the correlation df and filter by a minimum threshold, while\n",
    "eliminating one to avoid including the variable itself\n",
    "by converting it to a list, we can use it in our feature importance plots"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check for new dataframes\n",
    "\n",
    "vif_df = df_num[~df_num.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "\n",
    "X = vif_df\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "print(vif_data)\n",
    "\n",
    "corr_vif = vif_df.corr(method=\"spearman\").round(2)\n",
    "mask = np.triu(np.ones_like(corr_vif, dtype=bool))\n",
    "f, ax = plt.subplots(figsize=(18, 18))\n",
    "cmap = sns.diverging_palette(250, 1, as_cmap=True)\n",
    "sns.heatmap(corr, annot=True, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "corr_vif.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# keep Total Sales from the VIF calculation, Units due to its relationship to Total Sales\n",
    "# drop either Total Impressions or Impressions per Week\n",
    "# drop Any Promo % ACV because of its relationship to all other promos\n",
    "# corr = corr.drop(['Units', 'Impressions per Week',\n",
    "#                   'Any Promo %ACV', '%ACV Distribution'], axis=1)\n",
    "\n",
    "df_num = df_num.drop(['Units', 'Impressions per Week',\n",
    "                      'Any Promo %ACV', '%ACV Distribution'], axis=1)\n",
    "corr = df_num.corr(method=\"spearman\").round(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def imp_plots(target, features):\n",
    "    \"\"\"Form three importance plots\n",
    "\n",
    "    :param target:'dependent' component\n",
    "    :param features:'predictive' component\n",
    "    \"\"\"\n",
    "    target = target\n",
    "    df_all = df_num.dropna().astype(dtype='int32')\n",
    "    df_all = df_all[features + [target]]\n",
    "    df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "    X_train, y_train = df_train.drop(target, axis=1), df_train[target]\n",
    "    X_test, y_test = df_test.drop(target, axis=1), df_test[target]\n",
    "    rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                                max_features=1.0,\n",
    "                                min_samples_leaf=10, oob_score=True)\n",
    "    rf.fit(X_train, y_train)\n",
    "    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                           max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                           min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                           oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "    figure, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, figsize=(10, 10))\n",
    "    imp1 = importances(rf, X_test, y_test)\n",
    "    plot_importances(imp1, width=16, vscale=4, ax=ax1)\n",
    "\n",
    "    imp = pd.DataFrame()\n",
    "    imp['Feature'] = X_train.columns\n",
    "    imp['Importance'] = rf.feature_importances_\n",
    "    imp = imp.sort_values('Importance', ascending=False)\n",
    "    imp2 = imp.set_index('Feature')\n",
    "    plot_importances(imp2, width=16, vscale=4, ax=ax2)\n",
    "\n",
    "    perm_importance = permutation_importance(rf, X_test, y_test)\n",
    "    perm = pd.DataFrame()\n",
    "    perm['Feature'] = X_test.columns\n",
    "    perm['Importance'] = perm_importance.importances_mean\n",
    "    perm = perm.sort_values('Importance', ascending=False)\n",
    "    perm = perm.set_index('Feature')\n",
    "    plot_importances(perm, width=16, vscale=4, ax=ax3)\n",
    "    a = imp1.sort_values(by='Feature')\n",
    "    b = imp2.sort_values(by='Feature')\n",
    "    c = perm.sort_values(by='Feature')\n",
    "    d = (np.abs(a) + np.abs(b) + np.abs(c)).sort_values('Importance', ascending=False).mean(axis=1)\n",
    "    plt.show()\n",
    "    return d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following three importance plots look at different ways to measure importance\n",
    "in relation to predicting our variable of interest. We can continue this\n",
    "process many times to develop our Bayesian Hierarchy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales'] > .20) & (corr['Total Sales'] < 1.0)]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "features = corr_imp.index.tolist()\n",
    "imp_sales = imp_plots('Total Sales', features)\n",
    "print(imp_sales)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once the previous importance plots have given us insight into the most important\n",
    "variables at that level of the hierarchy, we can continue by choosing the most important\n",
    "for the next level of the hierarchy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[\n",
    "    abs(corr['Number of UPCs Selling'] > .20) & (corr['Number of UPCs Selling'] < 1.0)]\n",
    "corr_imp = corr_imp[['Number of UPCs Selling']]\n",
    "features = corr_imp.index.tolist()\n",
    "imp_UPC = imp_plots('Number of UPCs Selling', features)\n",
    "print(imp_UPC)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Price Decr Only %ACV'] > .20) & (corr['Price Decr Only %ACV'] < 1.0)]\n",
    "corr_imp = corr_imp[['Price Decr Only %ACV']]\n",
    "features = corr_imp.index.tolist()\n",
    "imp_price_decr = imp_plots('Price Decr Only %ACV', features)\n",
    "print(imp_price_decr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Any Promo Units'] > .20) & (corr['Any Promo Units'] < 1.0)]\n",
    "corr_imp = corr_imp[['Any Promo Units']]\n",
    "features = corr_imp.index.tolist()\n",
    "imp_promo_unit = imp_plots('Any Promo Units', features)\n",
    "print(imp_promo_unit)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Feat w/o Disp %ACV'] > .20) & (corr['Feat w/o Disp %ACV'] < 1.0)]\n",
    "corr_imp = corr_imp[['Feat w/o Disp %ACV']]\n",
    "features = corr_imp.index.tolist()\n",
    "imp_feat_no_disp = imp_plots('Feat w/o Disp %ACV', features)\n",
    "print(imp_feat_no_disp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This can be repeated for every variable of interest in the hierarchy\n",
    "The below kde plots are crucial to understanding the likelihood function\n",
    "distribution and beginning the Bayesian modeling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# final_vars = ['Number of UPCs Selling', 'Any Promo Units', '%ACV Distribution',\n",
    "#               'Feat w/o Disp %ACV', 'Price Decr Only %ACV', 'Disp w/o Feat %ACV',\n",
    "#               'Total Sales', 'Feat & Disp %ACV', 'Weeks', 'RMN','Tactic Category']\n",
    "\n",
    "final_vars = ['Number of UPCs Selling', 'Any Promo Units', '%ACV Distribution',\n",
    "              'Feat w/o Disp %ACV', 'Price Decr Only %ACV', 'Disp w/o Feat %ACV',\n",
    "              'Total Sales', 'Feat & Disp %ACV', 'Weeks', 'RMN']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_final = df[final_vars]\n",
    "\n",
    "for var in final_vars:\n",
    "    dist_test = df_final[var].dropna()\n",
    "    dist_test = dist_test.values\n",
    "    f = Fitter(dist_test, distributions=get_common_distributions(), timeout=45)\n",
    "    f.fit()\n",
    "    print(var)\n",
    "    print(f.summary())\n",
    "    print(f.get_best(method = 'sumsquare_error'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# az.plot_kde(df['Total Sales'].values, rug=True, label='Total Sales', figsize=(12, 8))\n",
    "# plt.yticks([0], alpha=0);\n",
    "# # most likely gamma"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# az.plot_kde(df['Number of UPCs Selling'].values, rug=True, label='Number of UPCs Selling', figsize=(12, 8))\n",
    "# plt.yticks([0], alpha=0);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# az.plot_kde(df['RMN'].values, rug=True, label='Any Promo Units', figsize=(12, 8))\n",
    "# plt.yticks([0], alpha=0);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# az.plot_kde(df['%ACV Distribution'].values, rug=True, label='%ACV Distribution', figsize=(12, 8))\n",
    "# plt.yticks([0], alpha=0);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# az.plot_kde(df['Feat w/o Disp %ACV'].values, rug=True, label='Feat w/o Disp %ACV', figsize=(12, 8))\n",
    "# plt.yticks([0], alpha=0);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# az.plot_kde(df['Price Decr Only %ACV'].values, rug=True, label='Price Decr Only %ACV', figsize=(12, 8))\n",
    "# plt.yticks([0], alpha=0);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# az.plot_kde(df['Disp w/o Feat %ACV'].values, rug=True, label='Disp w/o Feat %ACV', figsize=(12, 8))\n",
    "# plt.yticks([0], alpha=0);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# az.plot_kde(df['Feat & Disp %ACV'].values, rug=True, label='Feat & Disp %ACV', figsize=(12, 8))\n",
    "# plt.yticks([0], alpha=0);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# az.plot_kde(df['Weeks'].values, rug=True, label='Weeks', figsize=(12, 8))\n",
    "# plt.yticks([0], alpha=0);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# removing all zeroes for the initial testing\n",
    "\n",
    "df_sales_nz = df[df['Total Sales'] > 0.0]\n",
    "print(df_sales_nz[df_sales_nz['Total Sales'] == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model() as model_sales:\n",
    "    alpha = pm.Exponential('alpha', 100)\n",
    "    beta = pm.Exponential('beta', 1000)\n",
    "    g = pm.Gamma('g', alpha=alpha, beta=beta, observed=df_sales_nz['Total Sales'].values)\n",
    "    trace_sales = pm.sample(5000, tune=5000, return_inferencedata=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.plot_trace(trace_sales)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.summary(trace_sales)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.mcse(trace_sales)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.ess(trace_sales)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.plot_forest(trace_sales, var_names=['alpha', 'beta'], combined=True);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.plot_posterior(trace_sales, hdi_prob=0.99);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.plot_forest(trace_sales, r_hat=True);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.plot_energy(trace_sales);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ppc = pm.sample_posterior_predictive(trace_sales, samples=20000, model=model_sales)\n",
    "_, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.hist([g.mean() for g in ppc['g']], bins=19, alpha=0.5)\n",
    "ax.axvline(df_sales_nz['Total Sales'].mean())\n",
    "ax.set(title='Posterior predictive of the mean', xlabel='mean(x)', ylabel='Frequency');"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_final.columns = df_final.columns.str.replace('[#,@,&,%,''//'',\" \"]','')\n",
    "print(df_final.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_model = bmb.Model('TotalSales ~ NumberofUPCsSelling + AnyPromoUnits + ACVDistribution + FeatwoDispACV + PriceDecrOnlyACV + DispwoFeatACV + FeatDispACV + Weeks', data=df_final, dropna=True)\n",
    "\n",
    "test_fitted = test_model.fit(draws=1000, chains=4)\n",
    "test_model.predict(test_fitted, kind=\"pps\", draws=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.plot_trace(test_fitted, compact=False);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.summary(test_fitted)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.plot_ppc(test_fitted, figsize=(12, 12))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.plot_energy(test_fitted)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # check for new dataframes\n",
    "#\n",
    "# vif_df = df_final[~df_final.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "# vif_df = vif_df.drop(['Units', 'AnyPromoACV', 'TotalSales'], axis=1)\n",
    "# X = vif_df\n",
    "# vif_data = pd.DataFrame()\n",
    "# vif_data[\"feature\"] = X.columns\n",
    "# vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "# print(vif_data)\n",
    "#\n",
    "# corr = vif_df.corr(method=\"spearman\").round(2)\n",
    "# mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "# f, ax = plt.subplots(figsize=(18, 18))\n",
    "# cmap = sns.diverging_palette(250, 1, as_cmap=True)\n",
    "# sns.heatmap(corr, annot=True, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "#             square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "# corr.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# target = 'Total Sales'\n",
    "# CAT_FEATURES = ['Tactic Category']\n",
    "#\n",
    "# df_cat = df_final.dropna(subset=['Total Sales'])\n",
    "# # df_cat = df_cat[['Total Sales', 'Tactic Category']]\n",
    "# df_train, df_test = train_test_split(df_cat, test_size=0.3)\n",
    "# X_train, y_train = df_train.drop(target, axis=1), df_train[target]\n",
    "# X_test, y_test = df_test.drop(target, axis=1), df_test[target]\n",
    "# # X_train, y_train = df_train, df_train[target]\n",
    "# # X_test, y_test = df_test, df_test[target]\n",
    "#\n",
    "# catboost_model = CatBoostRegressor(n_estimators=200,\n",
    "#                                    loss_function = 'RMSE',\n",
    "#                                    eval_metric = 'RMSE',\n",
    "#                                    cat_features = CAT_FEATURES, one_hot_max_size=20)\n",
    "# catboost_model.fit(X_train, y_train, cat_features = CAT_FEATURES,\n",
    "#                    eval_set = (X_test, y_test),\n",
    "#                    use_best_model = True,\n",
    "#                    plot = True)\n",
    "# shap_values = catboost_model.get_feature_importance(Pool(\n",
    "#     X_train,\n",
    "#     label = y_train,\n",
    "#     cat_features = CAT_FEATURES\n",
    "# ),type = \"ShapValues\")\n",
    "# shap_values = shap_values[:,:-1]\n",
    "# shap.summary_plot(shap_values, X_train, max_display=50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "version": 1
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}