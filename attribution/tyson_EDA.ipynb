{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. Import data and necessary packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\norri\\.conda\\envs\\Attribution_EDA\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "The installed Theano(-PyMC) version (1.0.5) does not match the PyMC3 requirements.\n",
      "It was imported from ['C:\\\\Users\\\\norri\\\\.conda\\\\envs\\\\Attribution_EDA\\\\lib\\\\site-packages\\\\theano']\n",
      "For PyMC3 to work, a compatible Theano-PyMC backend version must be installed.\n",
      "See https://github.com/pymc-devs/pymc3/wiki for installation instructions.\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TheanoConfigParser' object has no attribute 'gcc__cxxflags'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 24>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcatboost\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CatBoostRegressor, Pool\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01marviz\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01maz\u001B[39;00m\n\u001B[1;32m---> 24\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpymc3\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpm\u001B[39;00m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtheano\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tensor \u001B[38;5;28;01mas\u001B[39;00m tt\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtheano\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m shared\n",
      "File \u001B[1;32m~\\.conda\\envs\\Attribution_EDA\\lib\\site-packages\\pymc3\\__init__.py:79\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     75\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m     78\u001B[0m _check_backend_version()\n\u001B[1;32m---> 79\u001B[0m \u001B[43m__set_compiler_flags\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     80\u001B[0m _hotfix_theano_printing()\n\u001B[0;32m     82\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpymc3\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m gp, ode, sampling\n",
      "File \u001B[1;32m~\\.conda\\envs\\Attribution_EDA\\lib\\site-packages\\pymc3\\__init__.py:61\u001B[0m, in \u001B[0;36m__set_compiler_flags\u001B[1;34m()\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__set_compiler_flags\u001B[39m():\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;66;03m# Workarounds for Theano compiler problems on various platforms\u001B[39;00m\n\u001B[1;32m---> 61\u001B[0m     current \u001B[38;5;241m=\u001B[39m \u001B[43mtheano\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgcc__cxxflags\u001B[49m\n\u001B[0;32m     62\u001B[0m     theano\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mgcc__cxxflags \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurrent\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m -Wno-c++11-narrowing\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'TheanoConfigParser' object has no attribute 'gcc__cxxflags'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from multiprocessing import Pool\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import arviz as az\n",
    "import pymc3 as pm\n",
    "from theano import tensor as tt\n",
    "from theano import shared\n",
    "from sklearn import preprocessing\n",
    "import shap\n",
    "import mkl\n",
    "import bambi as bmb\n",
    "import formulae\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from fitter import Fitter, get_common_distributions, get_distributions\n",
    "import xarray\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pymc3.variational.callbacks import CheckParametersConvergence\n",
    "\n",
    "# %load_ext heat\n",
    "# %load_ext line_profiler\n",
    "# %load_ext memory_profiler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/matt/Documents/cortex_Push.csv')\n",
    "df_bu = df\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pandas dataframes will have string, int, and float columns. The following\n",
    "three sections will look for columns that need to be fixed or ped altogether"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### The following few sections analyze categorical, float, and int variables\n",
    "##### so the dataset can be cleaned"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these drop column snippets are not used often here,\n",
    "# but have been useful, especially with large datasets\n",
    "\n",
    "df = df.drop(['Program Name', 'Retailers', 'Tactic', 'Vendor', 'Tactic Start Date',\n",
    "              'Tactic End Date'], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()\n",
    "print(df_cat.nunique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.RMN.replace(('Yes', 'No'), (1, 0), inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### In the below section we address the large number of missing s and also\n",
    "##### the columns consisting entirely of zeroes, and drop them accordingly"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "        ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "         '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "         'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "        ['ClientId', 'Program Id', 'TacticId', 'CategoryId',\n",
    "         'BrandId', 'Nielsen_Week_Year', 'VendorId'], axis=1)\n",
    "df_num = df.select_dtypes(exclude='object')\n",
    "df_num.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following heat maps are obviously important for understanding relationships,\n",
    "but more importantly their dataframes provide the ability to fill df.colnames\n",
    "that will be key to making a decision on what variables to explore for feature importance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This first heat map may become more important when categorical variables\n",
    "# are included in the model\n",
    "\n",
    "corr = df_num.corr(method=\"spearman\").round(2)\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "f, ax = plt.subplots(figsize=(18, 18))\n",
    "cmap = sns.diverging_palette(250, 1, as_cmap=True)\n",
    "sns.heatmap(corr, annot=True, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "corr.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we use the correlation df and filter by a minimum threshold, while\n",
    "eliminating one to avoid including the variable itself\n",
    "by converting it to a list, we can use it in our feature importance plots"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check for new dataframes\n",
    "vif_df = df_num[~df_num.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "\n",
    "X = vif_df\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "print(vif_data)\n",
    "\n",
    "corr_vif = vif_df.corr(method=\"spearman\").round(2)\n",
    "mask = np.triu(np.ones_like(corr_vif, dtype=bool))\n",
    "plt.subplots(figsize=(18, 18))\n",
    "cmap = sns.diverging_palette(250, 1, as_cmap=True)\n",
    "sns.heatmap(corr, annot=True, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "corr_vif.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df_num = df_num.drop(['Units', 'Impressions per Week',\n",
    "#                       'Any Promo %ACV', '%ACV Distribution'], axis=1)\n",
    "corr = df_num.corr(method=\"spearman\").round(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def imp_plots(target, predictors):\n",
    "    \"\"\"Form three importance plots\n",
    "\n",
    "    :param target:'dependent' component\n",
    "    :param predictors:'predictive' component\n",
    "    \"\"\"\n",
    "    target = target\n",
    "    df_all = df_num.dropna().astype(dtype='int32')\n",
    "    df_all = df_all[predictors + [target]]\n",
    "    df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "    X_train, y_train = df_train.drop(target, axis=1), df_train[target]\n",
    "    X_test, y_test = df_test.drop(target, axis=1), df_test[target]\n",
    "    rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                                max_features=1.0,\n",
    "                                min_samples_leaf=10, oob_score=True)\n",
    "    rf.fit(X_train, y_train)\n",
    "    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                           max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                           min_impurity_decrease=0.0, min_samples_leaf=10,\n",
    "                           min_samples_split=2,\n",
    "                           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                           oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "    figure, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, figsize=(10, 10))\n",
    "    imp1 = importances(rf, X_test, y_test)\n",
    "    plot_importances(imp1, width=16, vscale=4, ax=ax1)\n",
    "\n",
    "    imp = pd.DataFrame()\n",
    "    imp['Feature'] = X_train.columns\n",
    "    imp['Importance'] = rf.feature_importances_\n",
    "    imp = imp.sort_values('Importance', ascending=False)\n",
    "    imp2 = imp.set_index('Feature')\n",
    "    plot_importances(imp2, width=16, vscale=4, ax=ax2)\n",
    "\n",
    "    perm_importance = permutation_importance(rf, X_test, y_test)\n",
    "    perm = pd.DataFrame()\n",
    "    perm['Feature'] = X_test.columns\n",
    "    perm['Importance'] = perm_importance.importances_mean\n",
    "    perm = perm.sort_values('Importance', ascending=False)\n",
    "    perm = perm.set_index('Feature')\n",
    "    plot_importances(perm, width=16, vscale=4, ax=ax3)\n",
    "    a = imp1.sort_values(by='Feature')\n",
    "    b = imp2.sort_values(by='Feature')\n",
    "    c = perm.sort_values(by='Feature')\n",
    "    d = (np.abs(a) + np.abs(b) + np.abs(c)).sort_values('Importance',\n",
    "                                                        ascending=False).mean(axis=1)\n",
    "    plt.show()\n",
    "    return d"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following three importance plots look at different ways to measure importance\n",
    "in relation to predicting our variable of interest. We can continue this\n",
    "process many times to develop our Bayesian Hierarchy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales'] > .20) & (corr['Total Sales'] < 1.0)]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "features = corr_imp.index.tolist()\n",
    "imp_sales = imp_plots('Total Sales', features)\n",
    "print(imp_sales)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once the previous importance plots have given us insight into the most important\n",
    "variables at that level of the hierarchy, we can continue by choosing the most important\n",
    "for the next level of the hierarchy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Number of UPCs Selling'] > .20) & (corr['Number of UPCs Selling'] < 1.0)]\n",
    "corr_imp = corr_imp[['Number of UPCs Selling']]\n",
    "features = corr_imp.index.tolist()\n",
    "imp_UPC = imp_plots('Number of UPCs Selling', features)\n",
    "print(imp_UPC)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[\n",
    "    abs(corr['Price Decr Only %ACV'] > .20) & (corr['Price Decr Only %ACV'] < 1.0)]\n",
    "corr_imp = corr_imp[['Price Decr Only %ACV']]\n",
    "features = corr_imp.index.tolist()\n",
    "imp_price_decr = imp_plots('Price Decr Only %ACV', features)\n",
    "print(imp_price_decr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Any Promo Units'] > .20) & (corr['Any Promo Units'] < 1.0)]\n",
    "corr_imp = corr_imp[['Any Promo Units']]\n",
    "features = corr_imp.index.tolist()\n",
    "imp_promo_unit = imp_plots('Any Promo Units', features)\n",
    "print(imp_promo_unit)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[\n",
    "    abs(corr['Feat w/o Disp %ACV'] > .20) & (corr['Feat w/o Disp %ACV'] < 1.0)]\n",
    "corr_imp = corr_imp[['Feat w/o Disp %ACV']]\n",
    "features = corr_imp.index.tolist()\n",
    "imp_feat_no_disp = imp_plots('Feat w/o Disp %ACV', features)\n",
    "print(imp_feat_no_disp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This can be repeated for every variable of interest in the hierarchy\n",
    "The below kde plots are crucial to understanding the likelihood function\n",
    "distribution and beginning the Bayesian modeling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. This section continues on to modeling and exploration of modeling techniques"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_vars_cat = ['Number of UPCs Selling', 'Any Promo Units', '%ACV Distribution',\n",
    "                  'Feat w/o Disp %ACV', 'Price Decr Only %ACV', 'Disp w/o Feat %ACV',\n",
    "                  'Total Sales', 'Feat & Disp %ACV', 'RMN', 'Tactic Category', 'Brand']\n",
    "\n",
    "final_vars = ['Number of UPCs Selling', 'Any Promo Units', '%ACV Distribution',\n",
    "              'Feat w/o Disp %ACV', 'Price Decr Only %ACV', 'Disp w/o Feat %ACV',\n",
    "              'Total Sales', 'Feat & Disp %ACV', 'RMN']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_final = df[final_vars]\n",
    "\n",
    "dist_list = ['gamma', 'expon', 'cauchy', 'norm', 'uniform']\n",
    "\n",
    "for var in final_vars:\n",
    "    dist_test = df_final[var].dropna()\n",
    "    dist_test = dist_test.values\n",
    "    f = Fitter(dist_test, distributions=dist_list, timeout=60)\n",
    "    f.fit()\n",
    "    print(var)\n",
    "    print(f.summary(plot=False))\n",
    "    print(f.get_best(method='sumsquare_error'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2.1 Here I test the Bambi package based on PyMC3 to establish a baseline for a\n",
    "##### simpler model, though I use the same plots, more or less"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_nostr = df[final_vars]\n",
    "df_nostr = df_nostr[df_nostr['Total Sales'] > 0.0]\n",
    "df_nostr.columns = df_nostr.columns.str.replace('[#,@,&,%,''//'',\" \"]', '')\n",
    "\n",
    "df_nostr['TotalSales'] = np.log(df_nostr['TotalSales'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bmb_model = bmb.Model('TotalSales ~ NumberofUPCsSelling + '\n",
    "                      ' FeatwoDispACV + PriceDecrOnlyACV + DispwoFeatACV'\n",
    "                      , data=df_nostr, dropna=False)\n",
    "\n",
    "bmb_fitted = bmb_model.fit(draws=3000, tune=3000, chains=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2.3 Here I test and get diagnostics for this model\n",
    "##### and to prepare for the more complicated model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.plot_trace(bmb_fitted)\n",
    "print(az.summary(bmb_fitted))\n",
    "print(az.ess(bmb_fitted))\n",
    "az.plot_posterior(bmb_fitted, hdi_prob=0.99)\n",
    "az.plot_energy(bmb_fitted);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2.4 Note the above graph and compare it to the\n",
    "##### graph at the end. Having each chain closer to one\n",
    "##### is the ideal"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_final_cat = df_bu[final_vars_cat]\n",
    "df_final_cat = df_final_cat[df_final_cat['Total Sales'] > 0.0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following two charts show that many Tactic Category observations have dropped and two categories have also dropped."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()\n",
    "\n",
    "print('Original number of obs and Tactic Categories')\n",
    "print(df['Tactic Category'].count())\n",
    "print(df['Tactic Category'].nunique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df_final_cat.select_dtypes(include='object'):\n",
    "    if df_final_cat[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df_final_cat)\n",
    "        plt.show()\n",
    "\n",
    "print('Number of obs, new tactics, and new displays')\n",
    "print(df_final_cat['Tactic Category'].count())\n",
    "print(df_final_cat['Tactic Category'].nunique())\n",
    "print(df_final_cat['Brand'].nunique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tt.config.floatX = 'float64'\n",
    "\n",
    "df_final_cat = df_bu[final_vars_cat]\n",
    "df_final_cat = df_final_cat[df_final_cat['Total Sales'] > 0.0]\n",
    "\n",
    "df_final_cat['Total Sales'] = np.log(df_final_cat['Total Sales'])\n",
    "\n",
    "price_decr_idxs, price_decr = pd.factorize(df_final_cat['Price Decr Only %ACV'],\n",
    "                                           sort=True)\n",
    "tactic_idxs, tactics = pd.factorize(df_final_cat['Tactic Category'], sort=True)\n",
    "brand_idxs, brand = pd.factorize(df_final_cat['Brand'], sort=True)\n",
    "coords = {\"tactics\": tactics, 'brand': brand, 'obs_idx': np.arange(len(tactic_idxs)), 'price_decrease': price_decr}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Hierarchical Model with diagnostics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model(coords=coords) as hierarchical_model:\n",
    "    tactic_idx = pm.Data(\"Tactic Category\", tactic_idxs, dims=\"obs_idx\")\n",
    "    brand_idx = pm.Data(\"Brand\", brand_idxs, dims=\"obs_idx\")\n",
    "    price_decr = pm.Data(\"Price Decrease Only\", price_decr_idxs, dims=\"obs_idx\")\n",
    "\n",
    "    # Tactic Category hyperpriors parameters:\n",
    "    hp_b_0 = pm.Normal(\"hp_b_0\", mu=0.0, sigma=5.0)\n",
    "    sigma_a = pm.Exponential(\"sigma_a\", 1.0)\n",
    "    hp_b_1 = pm.Normal(\"hp_b_1\", mu=0.0, sigma=1.0)\n",
    "    sigma_b = pm.Exponential(\"sigma_b\", 0.5)\n",
    "\n",
    "    # Brand hyperpriors parameters\n",
    "    hp_b_2 = pm.Normal(\"hp_b_2\", mu=0.0, sigma=5.0)\n",
    "    sigma_c = pm.Exponential(\"sigma_c\", 1.0)\n",
    "    hp_b_3 = pm.Normal(\"hp_b_3\", mu=0.0, sigma=1.0)\n",
    "    sigma_d = pm.Exponential(\"sigma_d\", 0.5)\n",
    "\n",
    "    hp_b_4 = pm.Normal(\"hp_b_4\", mu=0.5, sigma=.3)\n",
    "\n",
    "    # estimates of independent intercepts and interactions of intercepts\n",
    "    b_0 = pm.Normal(\"tactic_int\", mu=hp_b_0, sigma=sigma_a, dims=\"tactics\")\n",
    "    b_1 = pm.Normal(\"tactic_slope\", mu=hp_b_1, sigma=sigma_b, dims=\"tactics\")\n",
    "\n",
    "    b_2 = pm.Normal(\"display_int\", mu=hp_b_2, sigma=sigma_c, dims=\"brand\")\n",
    "    b_3 = pm.Normal(\"display_slope\", mu=hp_b_3, sigma=sigma_d, dims=\"brand\")\n",
    "\n",
    "    price_decr_est = hp_b_4 * price_decr\n",
    "\n",
    "    # estimate of total sales using intercepts\n",
    "    sales_est_1 = b_0[tactic_idx] + b_1[tactic_idx] * brand_idx\n",
    "    sales_est_2 = b_2[brand_idx] + b_3[brand_idx] * tactic_idx\n",
    "    sales_est = sales_est_1 + sales_est_2 + price_decr_est\n",
    "\n",
    "    # Data likelihood\n",
    "    epsilon = pm.Exponential(\"noise\", 1.0)\n",
    "    Total_Sales = pm.Normal(\n",
    "            \"Log_Total_Sales\", mu=sales_est, sigma=epsilon,\n",
    "            observed=df_final_cat['Total Sales'], dims=\"obs_idx\")\n",
    "\n",
    "with hierarchical_model:\n",
    "    hierarchical_trace = pm.sample(draws=3000, init='advi+adapt_diag', chains=4,\n",
    "                                   tune=3000,\n",
    "                                   target_accept=0.98, return_inferencedata=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ppc = pm.sample_posterior_predictive(hierarchical_trace, samples=20481,\n",
    "                                     model=hierarchical_model)\n",
    "print('Model R-Squared')\n",
    "print(az.r2_score(df_final_cat['Total Sales'].values, ppc['Log_Total_Sales']))\n",
    "def scoreModel(trace,y,model_name):\n",
    "    ppc = pm.sample_posterior_predictive(hierarchical_trace, samples=20841,\n",
    "                                         model=hierarchical_model)\n",
    "    pred = ppc['Log_Total_Sales'].mean(axis=0)\n",
    "    mse = np.sqrt(mean_squared_error(y, pred))\n",
    "    print('The Mean Squared Error')\n",
    "    print(mse)\n",
    "\n",
    "scoreModel(hierarchical_trace, df_final_cat['Total Sales'], hierarchical_model)\n",
    "\n",
    "print('Trace Summary and Effective Sample Size')\n",
    "print(az.summary(hierarchical_trace, kind='stats'))\n",
    "print(az.summary(hierarchical_trace, kind='diagnostics'))\n",
    "az.plot_posterior(hierarchical_trace, hdi_prob=0.99)\n",
    "az.plot_energy(hierarchical_trace)\n",
    "plt.show()\n",
    "print('Bayesian fraction of missing information')\n",
    "print(az.bfmi(hierarchical_trace))\n",
    "az.plot_forest(hierarchical_trace, kind='ridgeplot')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(hierarchical_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Hierarchical Trace')\n",
    "az.waic(hierarchical_trace)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Bambi WAIC')\n",
    "az.waic(bmb_fitted)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compare_dict = {\"Bambi\": bmb_fitted, \"Hierarchical Trace\": hierarchical_trace}\n",
    "az.compare(compare_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idata = az.load_arviz_data('regression1d')\n",
    "x = xarray.DataArray(np.linspace(0, 1, 100))\n",
    "idata.posterior[\"y_model\"] = idata.posterior[\"intercept\"] + idata.posterior[\"slope\"]*x\n",
    "az.plot_lm(idata=idata, y=\"y\", x=x, figsize=(12, 12))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "version": 1
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}