---
title: "Robyn Test"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## STEP 0: Setup Environment

It's best to install and load in this order; it has yet to fail. The following two commands remove any previously installed H2O packages for R. Then we download packages H20 depends on.

```{r, echo=TRUE}
# pkgs <- c("RCurl","jsonlite")
# for (pkg in pkgs) {
#   if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
# }
```

Downloading and initializing H20 for R.

```{r, echo=TRUE}
# install.packages("h2o", type="source", repos="https://h2o-release.s3.amazonaws.com/h2o/rel-zygmund/4/R")
# install.packages("reticulate") # Install reticulate first if you haven't already
# remotes::install_github("facebookexperimental/Robyn/R")
# install.packages(c('nloptr', 'lares', 'dplyr', 'h2o', 'reticulate', 'tidyr'))
```

Calling the necessary libraries

```{r, echo=TRUE}
library(Robyn)
library(npreg)
library(dplyr)
library(nloptr)
library(lares)
library(tidyr)
library(stringr)
library(reticulate)
library(h2o)
```

Finally, initiate H20 and Nevergrad

```{r, echo=TRUE}
# h2o.init()
# conda_create("r-reticulate")
# conda_install("r-reticulate", "nevergrad", pip = TRUE)
# use_condaenv("r-reticulate")
```

## STEP 1: Data Creation

Ideal requirements:\
\* Two years' worth of data\
\* Consistent information on spend and impressions (FB prefers impressions over clicks)\
\* Several tactics with the associated information necessary.

Procedure: with a dataset such as the one described above, eliminate all but date, revenue, tactic category, tactic spend, and impressions or something similar, then create a pivot table that uses weekly dates as rows and has as columns a tactic category's spend and another for impressions. Revenue should also remain.

Robyn suggests a 10:1 ratio between observations and variables, so you'll likely have more tactics than you need and have to cull them down.

Robyn does not like nulls, nor too many zeroes, and if imputing missing values, you need to be very deliberate on how you do it. Multicollinearity is a concern, but less so with ridge regression, so you have a bit of wiggle room there. Just expect to have many of your models fail to converge before you start finding some that do.

## STEP 2: Load Data

This section is important not only to load the data, but it also tells the program where to place the Robyn object.

```{r, echo=TRUE}
setwd('C:/Users/norri/Desktop/')
Sys.setenv(R_FUTURE_FORK_ENABLE = TRUE) # Force multicore when using RStudio
options(future.fork.enable = TRUE)
df <- read.csv('robyn_84.csv', fileEncoding = 'UTF-8-BOM')

data("dt_prophet_holidays")
head(dt_prophet_holidays)
```

I've found it's best to set the full path to make sure this file is in the right spot and can be overwritten.

```{r, echo=TRUE}
robyn_object <- "C:/Users/norri/Desktop/MyRobyn.RDS"
```

## STEP 2a: Four steps of model specification

### 2a-1: First, specify the input variables.

All sign control are now automatically provided: "positive" for media & organic variables and "default" for all others. Users can still customize signs if necessary. Documentation is available in ?robyn_inputs.

```{r, echo=TRUE}
InputCollect <- robyn_inputs(
  dt_input = df,
  dt_holidays = dt_prophet_holidays,
  date_var = "DATE", # date format must be "2020-01-01, and must be in all caps"
  dep_var = "revenue", # there should be only one dependent variable
  dep_var_type = "revenue", # "revenue" (ROI) or "conversion" (CPA)
  prophet_vars = c("trend", "season", "holiday"), # "trend","season", "weekday" & "holiday"
  prophet_country = "US", # input one country. dt_prophet_holidays includes 59 countries by default
  context_vars = c("cargill", 'inflation'), # e.g. competitors, discount, unemployment etc
  paid_media_spends = c("banner_S", "blog_S", "coupon_S", "email_S"), # mandatory input
  paid_media_vars = c("banner_I", "blog_I", "coupon_I", "email_I"), # mandatory.
  # paid_media_vars must have same order as paid_media_spends. Use media exposure metrics like
  # impressions, GRP etc. If not applicable, use spend instead.
  # organic_vars = "newsletter", # marketing activity without media spend
  # factor_vars = c("events"), # force variables in context_vars or organic_vars to be categorical
  window_start = "2019-10-14",
  window_end = "2021-4-26",
  adstock = "weibull_pdf" # geometric, weibull_cdf or weibull_pdf.
)
print(InputCollect)
```

### 2a-2: Define and add hyperparameters

Robyn's hyperparameters have four components: Adstock parameters (theta or shape/scale). Saturation parameters (alpha/gamma). Regularisation parameter (lambda). No need to specify manually. Time series validation parameter (train_size).

```{r, echo=TRUE}
names = hyper_names(adstock = InputCollect$adstock, all_media = InputCollect$all_media)
limit = hyper_limits()

print(length(hyper_names(adstock = InputCollect$adstock, all_media = InputCollect$all_media)))
alphas = c(.1, 9.9)
gammas = c(.1, 1)
shapes = c(0, 19.9)
scales = c(0, 1)

hyperparameters <-list(
  banner_S_alphas = c(.1, 9.9)
  , banner_S_gammas = c(.1, 1)
  , banner_S_shapes = c(0, 19.9)
  , banner_S_scales = c(0, 1)
  
  , blog_S_alphas = c(.1, 9.9)
  , blog_S_gammas = c(.1, 1)
  , blog_S_shapes = c(0, 19.9)
  , blog_S_scales = c(0, 1)
  
  , coupon_S_alphas = c(.1, 9.9)
  , coupon_S_gammas = c(.1, 1)
  , coupon_S_shapes = c(0, 19.9)
  , coupon_S_scales = c(0, 1)
  
  , email_S_alphas = c(.1, 9.9)
  , email_S_gammas = c(.1, 1)
  , email_S_shapes = c(0, 19.9)
  , email_S_scales = c(0, 1)
  
  , train_size = c(0.5, 0.8)
)

```

### 2a-3: Put Hyperparameters into robyn_input()

```{r, echo=TRUE}
InputCollect <- robyn_inputs(InputCollect = InputCollect,
                             hyperparameters = hyperparameters)
```

### 2a-4: Optional model calibration

1.  Calibration channels need to be paid_media_spends or organic_vars names.
2.  We strongly recommend to use Weibull PDF adstock for more degree of freedom when calibrating Robyn.
3.  We strongly recommend to use experimental and causal results that are considered ground truth to calibrate MMM. Usual experiment types are identity-based (e.g. Facebook conversion lift) or geo-based (e.g. Facebook GeoLift). Due to the nature of treatment and control groups in an experiment, the result is considered immediate effect. It's rather impossible to hold off historical carryover effect in an experiment. Therefore, only calibrates the immediate and the future carryover effect. When calibrating with causal experiments, use calibration_scope = "immediate".
4.  It's controversial to use attribution/MTA contribution to calibrate MMM. Attribution is considered biased towards lower-funnel channels and strongly impacted by signal quality. When calibrating with MTA, use calibration_scope = "immediate".
5.  Every MMM is different. It's highly contextual if two MMMs are comparable or not. In case of using other MMM result to calibrate Robyn, use calibration_scope = "total".
6.  Currently, Robyn only accepts point-estimate as calibration input. For example, if 10k\$ spend is tested against a hold-out for channel A, then input the incremental return as point-estimate as the example below.
7.  The point-estimate has to always match the spend in the variable. For example, if channel A usually has \$100K weekly spend and the experimental holdout is 70%, input the point-estimate for the 30K, not the 70K.
8.  If an experiment contains more than one media variable, input "channel_A+channel_B" to indicate combination of channels, case sensitive.

```{r, echo=TRUE}
# calibration_input <- data.frame(
#   # channel name must in paid_media_vars
#   channel = c(),
#   # liftStartDate must be within input data range
#   liftStartDate = as.Date(c()),
#   # liftEndDate must be within input data range
#   liftEndDate = as.Date(c()),
#   # Provided value must be tested on same campaign level in model and same metric as dep_var_type
#   liftAbs = c(),
#   # Spend within experiment: should match within a 10% error your spend on date range for each channel from dt_input
#   spend = c(),
#   # Confidence: if frequentist experiment, you may use 1 - pvalue
#   confidence = c(),
#   # KPI measured: must match your dep_var
#   metric = c("revenue", "revenue", "revenue", "revenue"),
#   # Either "immediate" or "total". For experimental inputs like Facebook Lift, "immediate" is recommended.
#   calibration_scope = c("immediate", "immediate", "immediate", "immediate")
# )
# InputCollect <- robyn_inputs(InputCollect = InputCollect, calibration_input = calibration_input)
```

## STEP 3: Build Initial Model

Run all trials and iterations. Use ?robyn_run to check parameter definition

```{r, echo=TRUE}

OutputModels <- robyn_run(
  InputCollect = InputCollect # feed in all model specification
  , cores = NULL # on Windows, it will only use 1, multi-core on Linux
  , iterations = 2500 # Increase if failing to converge
  , trials = 10 # Increase if failing to converge
  # , outputs = FALSE # outputs = FALSE disables direct model output
  # , ts_validation = TRUE, # 3-way-split time series for NRMSE validation.
)
print(OutputModels)
```

```{r}
## Check time-series validation plot (when ts_validation == TRUE)
# Read more and replicate results: ?ts_validation
if (OutputModels$ts_validation) OutputModels$ts_validation_plot

## Check MOO (multi-objective optimization) convergence plots
OutputModels$convergence$moo_distrb_plot
OutputModels$convergence$moo_cloud_plot
```

Calculate Pareto optimality, cluster and export results and plots. See ?robyn_outputs

```{r, echo=TRUE}

OutputCollect <- robyn_outputs(
  InputCollect, OutputModels
  , pareto_fronts = 'auto'
  , calibration_constraint = .1 #& default at 0.1
  , csv_out = "pareto" # "pareto" or "all"
  , clusters = TRUE # Set to TRUE to cluster similar models by ROAS. See ?robyn_clusters
  , plot_pareto = TRUE # Set to FALSE to deactivate plotting and saving model one-pagers
  , plot_folder = robyn_object # path for plots export
  , export = TRUE
)
print(OutputCollect)
```

## STEP 4: Select and save model of choice

```{r}
## Compare all model one-pagers and select one that mostly reflects your business reality
print(OutputCollect)
select_model <- "7_192_3" # select one from above
ExportedModel <- robyn_write(InputCollect, OutputCollect, select_model)
print(ExportedModel)
```

## STEP 5: Get budget allocation from model above

Budget allocation result requires further validation. Please use this recommendation with caution. Don't interpret budget allocation result if selected model above doesn't meet business expectation. Look at any and all suggestions to see if the recomendations make sense; On occasions, they suggest dropping spend on the holidays and even no ad spend Check media summary for selected model Run ?robyn_allocator to check parameter definition Run the "max_historical_response" scenario: "What's the revenue lift potential with the same historical spend level and what is the spend mix?"

```{r, echo=TRUE}
AllocatorCollect1 <- robyn_allocator(
  InputCollect = InputCollect,
  OutputCollect = OutputCollect,
  select_model = select_model,
  scenario = "max_historical_response",
  channel_constr_low = 0.7,
  channel_constr_up = c(1.2, 1.5, 1.5, 1.5),
  export = TRUE,
  date_min = "2019-12-14",
  date_max = "2021-4-26"
)
print(AllocatorCollect1)
# plot(AllocatorCollect1)
```

Run the "max_response_expected_spend" scenario: "What's the maximum response for a given total spend based on historical saturation and the spend mix?" "optmSpendShareUnit" is the optimum spend share.

```{r, echo=TRUE}
AllocatorCollect2 <- robyn_allocator(
  InputCollect = InputCollect,
  OutputCollect = OutputCollect,
  select_model = select_model,
  scenario = "max_response_expected_spend",
  channel_constr_low = c(0.7, 0.7, 0.7, 0.7),
  channel_constr_up = c(1.2, 1.5, 1.5, 1.5),
  expected_spend = 1000000, # Total spend to be simulated
  expected_spend_days = 7, # Duration of expected_spend in days
  export = TRUE
)
print(AllocatorCollect2)
AllocatorCollect2$dt_optimOut
# plot(AllocatorCollect2)
```

```{r, echo=TRUE}
## QA optimal response
# Pick any media variable: InputCollect$all_media
select_media <- "banner_S"
# For paid_media_spends set metric_value as your optimal spend
metric_value <- AllocatorCollect1$dt_optimOut$optmSpendUnit[
  AllocatorCollect1$dt_optimOut$channels == select_media
]; metric_value
# # For paid_media_vars and organic_vars, manually pick a value
# metric_value <- 10000

if (TRUE) {
  optimal_response_allocator <- AllocatorCollect1$dt_optimOut$optmResponseUnit[
    AllocatorCollect1$dt_optimOut$channels == select_media
  ]
  optimal_response <- robyn_response(
    InputCollect = InputCollect,
    OutputCollect = OutputCollect,
    select_model = select_model,
    select_build = 0,
    media_metric = select_media,
    metric_value = metric_value
  )
  plot(optimal_response$plot)
  if (length(optimal_response_allocator) > 0) {
    cat("QA if results from robyn_allocator and robyn_response agree: ")
    cat(round(optimal_response_allocator) == round(optimal_response$response), "( ")
    cat(optimal_response$response, "==", optimal_response_allocator, ")\n")
  }
}
```

## STEP 6: Model Refresh

```{r}

## Must run robyn_write() (manually or automatically) to export any model first, before refreshing.
## The robyn_refresh() function is suitable for updating within "reasonable periods".
## Two situations are considered better to rebuild model:
## 1. most data is new. If initial model has 100 weeks and 80 weeks new data is added in refresh,
## it might be better to rebuild the model. Rule of thumb: 50% of data or less can be new.
## 2. new variables are added.

# Provide JSON file with your InputCollect and ExportedModel specifications
```

## STEP 7: Get budget recommendation based on refresh

```{r}
# Run ?robyn_allocator to check parameter definition
# AllocatorCollect <- robyn_allocator(
#   InputCollect = InputCollect,
#   OutputCollect = OutputCollect,
#   select_model = select_model,
#   scenario = "max_response_expected_spend",
#   channel_constr_low = c(0.7, 0.7, 0.7, 0.7, 0.7),
#   channel_constr_up = c(1.2, 1.5, 1.5, 1.5, 1.5),
#   expected_spend = 2000000, # Total spend to be simulated
#   expected_spend_days = 14 # Duration of expected_spend in days
# )
# print(AllocatorCollect)
# # plot(AllocatorCollect)
```

## STEP 8: Get MROI

```{r, echo=TRUE}

## -------------------------------- NOTE v3.6.0 CHANGE !!! --------------------------- ##
## The robyn_response() function can now output response for spends and exposures (imps,
## GRP, circular_S sendings etc.) as well as plotting individual saturation curves. New
## argument names "media_metric" and "metric_value" instead "paid_media_var" and "spend"
## are now used to accommodate this change. Also the returned output is a list now and
## contains also the plot.
## ------------------------------------------------------------------------------------##

# Get response for 80k from result saved in robyn_object
Spend1 <- 60000
Response1 <- robyn_response(
  InputCollect = InputCollect,
  OutputCollect = OutputCollect,
  select_model = select_model,
  media_metric = "banner_S",
  metric_value = Spend1
)
Response1$response / Spend1 # ROI for search 80k
Response1$plot

### Or you can call a JSON file directly (a bit slower)
# Response1 <- robyn_response(
#   json_file = json_file,
#   dt_input = dt_simulated_weekly,
#   dt_holidays = dt_prophet_holidays,
#   media_metric = "search_S",
#   metric_value = Spend1
# )

# Get response for +10%
Spend2 <- Spend1 * 1.1
Response2 <- robyn_response(
  InputCollect = InputCollect,
  OutputCollect = OutputCollect,
  select_model = select_model,
  media_metric = "banner_S",
  metric_value = Spend2
)
Response2$response / Spend2 # ROI for search 81k
Response2$plot

# Marginal ROI of next 1000$ from 80k spend level for search
(Response2$response - Response1$response) / (Spend2 - Spend1)

## Example of getting paid media exposure response curves
imps <- 50000000
response_imps <- robyn_response(
  InputCollect = InputCollect,
  OutputCollect = OutputCollect,
  select_model = select_model,
  media_metric = "banner_I",
  metric_value = imps
)
response_imps$response / imps * 1000
response_imps$plot

## Example of getting organic media exposure response curves
# sendings <- 30000
# response_sending <- robyn_response(
#   InputCollect = InputCollect,
#   OutputCollect = OutputCollect,
#   select_model = select_model,
#   media_metric = "newsletter",
#   metric_value = sendings
# )
# response_sending$response / sendings * 1000
# response_sending$plot
```

## Optional: Recreate Robyn Model

```{r}
# # From an exported JSON file (which is created automatically when exporting a model)
# # we can re-create a previously trained model and outputs. Note: we need to provide
# # the main dataset and the holidays dataset, which are NOT stored in the JSON file.
# # These JSON files will be automatically created in most cases.
# 
# ############ WRITE ############
# # Manually create JSON file with inputs data only
# robyn_write(InputCollect, dir = "~/Desktop")
# 
# # Manually create JSON file with inputs and specific model results
# robyn_write(InputCollect, OutputCollect, select_model)
# 
# 
# ############ READ ############
# # Recreate `InputCollect` and `OutputCollect` objects
# # Pick any exported model (initial or refreshed)
# json_file <- "~/Desktop/Robyn_202208231837_init/RobynModel-1_100_6.json"
# 
# # Optional: Manually read and check data stored in file
# json_data <- robyn_read(json_file)
# print(json_data)
# 
# # Re-create InputCollect
# InputCollectX <- robyn_inputs(
#   dt_input = dt_simulated_weekly,
#   dt_holidays = dt_prophet_holidays,
#   json_file = json_file)
# 
# # Re-create OutputCollect
# OutputCollectX <- robyn_run(
#   InputCollect = InputCollectX,
#   json_file = json_file,
#   export = FALSE)
# 
# # Or re-create both by simply using robyn_recreate()
# RobynRecreated <- robyn_recreate(
#   json_file = json_file,
#   dt_input = dt_simulated_weekly,
#   dt_holidays = dt_prophet_holidays,
#   quiet = FALSE)
# InputCollectX <- RobynRecreated$InputCollect
# OutputCollectX <- RobynRecreated$OutputCollect
# 
# # Re-export model and check summary (will get exported in your current working directory)
# myModel <- robyn_write(InputCollectX, OutputCollectX, dir = "~/Desktop")
# print(myModel)
# 
# # Re-create one-pager
# myModelPlot <- robyn_onepagers(InputCollectX, OutputCollectX, select_model = NULL, export = FALSE)
# # myModelPlot$`1_204_5`$patches$plots[[6]]
# 
# # Refresh any imported model
# RobynRefresh <- robyn_refresh(
#   json_file = json_file,
#   dt_input = InputCollectX$dt_input,
#   dt_holidays = InputCollectX$dt_holidays,
#   refresh_steps = 6,
#   refresh_mode = "manual",
#   refresh_iters = 1000,
#   refresh_trials = 1
# )
# 
# # Recreate response curves
# robyn_response(
#   InputCollect = InputCollectX,
#   OutputCollect = OutputCollectX,
#   media_metric = "newsletter",
#   metric_value = 50000
# )
```

## Glossary

3.  Hyperparameter interpretation & recommendation:

Geometric adstock: Theta is the only parameter and means fixed decay rate. Assuming TV spend on day 1 is 100€ and theta = 0.7, then day 2 has 100 x 0.7=70 worth of effect carried-over from day 1, day 3 has 70\*0.7=49€ from day 2 etc. Rule-of-thumb for common media genre: TV c(0.3, 0.8), OOH/Print/Radio c(0.1, 0.4), digital c(0, 0.3)

Weibull CDF adstock: The Cumulative Distribution Function of Weibull has two parameters , shape & scale, and has flexible decay rate, compared to Geometric adstock with fixed decay rate. The shape parameter controls the shape of the decay curve. Recommended bound is c(0.0001, 2). The larger the shape, the more S-shape. The smaller, the more L-shape. Scale controls the inflexion point of the decay curve. We recommend very conservative bounce of c(0, 0.1), because scale increases the adstock half-life greatly.

Weibull PDF adstock: The Probability Density Function of the Weibull also has two parameters, shape & scale, and also has flexible decay rate as Weibull CDF. The difference is that Weibull PDF offers lagged effect. When shape \> 2, the curve peaks after x = 0 and has NULL slope at x = 0, enabling lagged effect and sharper increase and decrease of adstock, while the scale parameter indicates the limit of the relative position of the peak at x axis; when 1 \< shape \< 2, the curve peaks after x = 0 and has infinite positive slope at x=0, enabling lagged effect and slower increase and decrease of adstock, while scale has the same effect as above; when shape = 1, the curve peaks at x = 0 and reduces to exponential decay, while scale controls the inflexion point; when 0 \< shape \< 1, the curve peaks at x = 0 and has increasing decay, while scale controls the inflexion point. When all possible shapes are relevant, we recommend c(0.0001, 10) as bounds for shape; when only strong lagged effect is of interest, we recommend c(2.0001, 10) as bound for shape. In all cases, we recommend conservative bound of c(0, 0.1) for scale. Due to the great flexibility of Weibull PDF, meaning more freedom in hyperparameter spaces for Nevergrad to explore, it also requires larger iterations to converge.

Hill function for saturation: Hill function is a two-parametric function in Robyn with alpha and gamma. Alpha controls the shape of the curve between exponential and s-shape. Recommended bound is c(0.5, 3). The larger the alpha, the more S-shape. The smaller, the more C-shape. Gamma controls the inflexion point. Recommended bounce is c(0.3, 1). The larger the gamma, the later the inflection point in the response curve.

4.  Set individual hyperparameter bounds. They either contain two values e.g. c(0, 0.5), or only one value, in which case you'd "fix" that hyperparameter.
