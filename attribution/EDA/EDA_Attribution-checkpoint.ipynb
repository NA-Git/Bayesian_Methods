{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug\n",
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups.\n",
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_cat.describe()\n",
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model\n",
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model\n",
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior\n",
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important\n",
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot\n",
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy\n",
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()segment = [var for var in df.columns if df[var].dtype=='O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() + df[segment].isna().sum())\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "unique_cats = df_cat.nunique()\n",
    "\n",
    "# I like this visualization but it needs some tweaking or replacement\n",
    "# note: write a function because this repetitive plotting could be done in a few lines\n",
    "# for col in segment:\n",
    "#     sns.boxplot(y=df_cat.nunique().astype('int32'), x=col, data=df_cat)\n",
    "#     plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()integer = [var for var in df.columns if df[var].dtype=='int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()fp = [var for var in df.columns if df[var].dtype=='float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "print(df[fp].isnull().sum() + df[fp].isna().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()# in addition to the kde plots at the bottom of the notebook, this give some\n",
    "# idea as to the distributions for the likelihood functions\n",
    "df.hist(figsize=(25, 25), bins=20)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()missing_segment = (df[segment].isnull().sum() / len(df)) * 100\n",
    "missing_integer = (df[integer].isnull().sum() / len(df)) * 100\n",
    "missing_float = (df[fp].isnull().sum() / len(df)) * 100\n",
    "print(missing_segment)\n",
    "print(missing_integer)\n",
    "print(missing_float)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()print('Missing variables check' + ' ' + str((len(integer) + len(fp) +\n",
    "                                             len(segment) - len(df.T))))\n",
    "print(df.info())\n",
    "print(df.describe())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()cols = 5\n",
    "rows = 10\n",
    "num_cols = df.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 6, rows * 6))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df.corr(method=\"spearman\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()# print(len(df))\n",
    "# df = df.dropna(how='any', subset=['Impressions per Week'])\n",
    "# df['Impressions per Week'].astype('float32', copy=True)\n",
    "# df = df.dropna(how='any', subset=['%ACV Distribution'])\n",
    "# df['%ACV Distribution'].astype('float32', copy=True)\n",
    "# df = df.dropna(how='any', subset=['Units'])\n",
    "# df['Units'].astype('float32', copy=True)\n",
    "# df = df.dropna(how='any', subset=['Number of UPCs Selling'])\n",
    "# df['Number of UPCs Selling'].astype('float32', copy=True)\n",
    "# df = df.dropna(how='any', subset=['Total Sales'])\n",
    "# df['Total Sales'].astype('float32', copy=True)\n",
    "# df = df.dropna(how='any', subset=['Base $'])\n",
    "# df['Base $'].astype('float32', copy=True)\n",
    "# print(df.describe())\n",
    "# print(df.info())\n",
    "# ['Impressions per Week', 'Units', '%ACV Distribution',\n",
    "#  'Number of UPCs Selling', 'Base $']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()# most of the EDA techniques we'll use cannot handle categorical data, so dropping\n",
    "# the categoricals and rerunning the correlation plot is valuable here\n",
    "df_num = df.select_dtypes(exclude=object)\n",
    "corr = df_num.corr()\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()# this is just another visualization of correlation that I like, I may switch it with others\n",
    "viz = plot_corr_heatmap(df_num, figsize=(16,17))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Total Sales', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Impressions per Week', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Brand Share of Tactic Redemption Cost', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 1], x='%ACV Distribution', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 2], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()df_all = df_num.dropna().astype(dtype='int32')\n",
    "\n",
    "features = ['Impressions per Week', 'Brand Share of Tactic Redemption Cost',\n",
    "            'Any Promo Units', '%ACV Distribution', 'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales',axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales',axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            #                            max_features=X_train.shape[1]-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()I = importances(rf, X_test, y_test)\n",
    "plot_importances(I)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = df_all.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 250:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import linregress\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/cortex_Push.csv')\n",
    "df.describe()\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop([], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df = df.drop(\n",
    "#     [], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Base $', 'Incr $', 'Base Units', 'Incr Units',\n",
    "     '$ Shr - Ty Subcategory', 'Units Shr - Ty Category',\n",
    "     'Units Shr - Ty Subcategory'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Total Sales']) > .2]\n",
    "corr_imp = corr_imp[['Total Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Any Promo Units', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Disp w/o Feat %ACV', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Number of UPCs Selling', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Units', 'Any Promo Units',\n",
    "            'Disp w/o Feat %ACV',\n",
    "            'Number of UPCs Selling']\n",
    "target = 'Total Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Total Sales', axis=1), df_train['Total Sales']\n",
    "X_test, y_test = df_test.drop('Total Sales', axis=1), df_test['Total Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 25:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()X = np.array(df_num['Total Sales'])\n",
    "Y = np.array(df_num['Any Promo Units'])\n",
    "Z = np.array(df_num['%ACV Distribution'])\n",
    "temp = np.vstack((X, Y))\n",
    "letters = (np.vstack((temp, Z))).T\n",
    "\n",
    "# pca = PCA(n_components=3, svd_solver='full')\n",
    "# plt(pca.fit(letters))\n",
    "\n",
    "# lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "# X_r2 = lda.fit(X, Y).transform(X)\n",
    "#\n",
    "# X_r.plt()\n",
    "# X_r2.plt()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "version": 1
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}