{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "MD"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### The purpose of this EDA notebook is the following:\n",
    "- Better understand the nature of the relationship between the independent variables\n",
    "- Create an initial model with reasonable economic assumptions that may be dropped in later versions\n",
    "- Explore methods of imputation for missing variables to provide more data samples\n",
    "- Avoid linear combinations that might be more difficult to spot in the Bayesian Modeling process\n",
    "- Establish a reasonable measure of variable importance, which along with correlation plots may inform initial hierarchies\n",
    "- Create visualizations of poor quality data and also establish probability distributions for the likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'category_encoders'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 21>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StandardScaler\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minspection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m permutation_importance\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcategory_encoders\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mce\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'category_encoders'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "from rfpimp import *\n",
    "from rfpimp import plot_corr_heatmap\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "import category_encoders as ce"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### In an effort to generalize for future EDA, 'df' makes it easier to debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "type": "CODE"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/norri/Desktop/all_ab_data.csv')\n",
    "df.describe()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The info function above tells us we have three data types and counts of each. The next section will explore those variables in groups."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(['CONCAT', 'Brewer Value'], axis=1)\n",
    "segment = [var for var in df.columns if df[var].dtype == 'O']\n",
    "print('There are {} categorical variables\\n'.format(len(segment)))\n",
    "print('The categorical variables are :\\n\\n', segment)\n",
    "print(df[segment].isnull().sum() / len(df))\n",
    "df_cat = df.select_dtypes(include=object)\n",
    "df_cat.info()\n",
    "df_cat.describe()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['F81', 'Product Development Index', 'Sum of Dollar Sales Checkout Display',\n",
    "     'Sum of Dollar Sales Outside Display', 'Sum of Dollar Sales Signage and Feature',\n",
    "     'Sum of Dollar Sales Signage, Feature and Display'], axis=1)\n",
    "integer = [var for var in df.columns if df[var].dtype == 'int64']\n",
    "print('There are {} integer variables\\n'.format(len(integer)))\n",
    "print('The integer variables are :\\n\\n', integer)\n",
    "print(df[integer].isnull().sum())\n",
    "df_int = df.select_dtypes(include=int)\n",
    "if len(df_int.columns) > 0.0:\n",
    "    df_int.info()\n",
    "    df_int.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    ['Unit Share of SubCategory', 'Dollar Share of SubCategory',\n",
    "     'Incremental Dollars', 'Incremental Dollars % Change vs YA',\n",
    "     'Incremental Units', 'Incremental Units % Change vs YA',\n",
    "     '% Increase in Dollars by Merch Any Special Pack',\n",
    "     '% Increase in Dollars by Merch Special Pack Only',\n",
    "     '% Increase in Units by Merch',\n",
    "     '% Increase in Units by Merch Any Special Pack',\n",
    "     '% Increase in Units by Merch Special Pack Only',\n",
    "     '% Incremental Units by Merch Any Special Pack',\n",
    "     '% Incremental Units by Merch Special Pack Only',\n",
    "     '% Increase in Units by Merch No Merch'], axis=1)\n",
    "fp = [var for var in df.columns if df[var].dtype == 'float64']\n",
    "print('There are {} float variables\\n'.format(len(fp)))\n",
    "print('The float variables are :\\n\\n', fp)\n",
    "fp_na = df[fp].isnull().sum() / len(df) * 100\n",
    "print(fp_na[fp_na > 10])\n",
    "fp_zero = df[fp].sum()\n",
    "print(fp_zero[fp_zero == 0.0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# agreed upon drops\n",
    "df = df.drop(\n",
    "    ['Dollar Sales', 'Sum of Base Dollar Sales',\n",
    "     'Sum of Incremental Dollars', 'Base Dollar Sales',\n",
    "     'Base Dollar Sales % Change vs YA', 'Base Unit Sales'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# can drop year ago columns\n",
    "df_fp = df.select_dtypes('float')\n",
    "df_num = df_fp\n",
    "df_fp.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clearly the float variables contain all the missing data in the dataset. In\n",
    "### cases like this, dropping the missing values are a trade-off to consider\n",
    "### against dropping an entire column and losing its input into the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this section should come later; for larger datasets takes too long to run\n",
    "# with too little return in information\n",
    "cols = 5\n",
    "rows = 20\n",
    "num_cols = df_num.select_dtypes(exclude='object').columns\n",
    "fig = plt.figure(figsize=(cols * 5, rows * 5))\n",
    "for i, col in enumerate(num_cols):\n",
    "    ax = fig.add_subplot(rows, cols, i + 1)\n",
    "    sns.histplot(x=df[col], ax=ax)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# these visualizations provide some clue as to how to model the large number\n",
    "# of variables and to examine their relationships\n",
    "corr = df_num.corr(method=\"pearson\")\n",
    "corr.style.background_gradient(cmap=\"coolwarm\").set_precision(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Imputation of missing values is too unreliable to base the rest of the\n",
    "##### model on. Later tests will tell if there is any bias present. This is when\n",
    "##### the most standout variables should be chosen for the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is just another visualization of correlation that I like, I may switch it with others\n",
    "# is nice because it is easier to zoom in for inspection\n",
    "viz = plot_corr_heatmap(df_num, figsize=(20, 20))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corr_imp = corr[abs(corr['Sum of Dollar Sales']) >.5]\n",
    "corr_imp = corr_imp[['Sum of Dollar Sales']]\n",
    "print(corr_imp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 3, figsize=(15, 15))\n",
    "sns.kdeplot(ax=axes[0, 0], x='Sum of Dollar Sales', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 1], x='Dollar Share of Category', data=df_num)\n",
    "sns.kdeplot(ax=axes[0, 2], x='Sum of Dollar Sales Any Display', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 0], x='Sum of Dollar Sales Any Merch', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 1], x='Sum of Dollar Sales No Merch (non-promo)', data=df_num)\n",
    "sns.kdeplot(ax=axes[1, 2], x='Sum of Dollar Sales Any Price Reduction', data=df_num)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### These kde plots will a primary tool in determining the likelihood\n",
    "### distributions and giving information on the prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = ['Sum of Dollar Sales Any Merch', 'Dollar Share of Category',\n",
    "            'Sum of Dollar Sales Any Display',\n",
    "            'Sum of Dollar Sales No Merch (non-promo)',\n",
    "            'Sum of Dollar Sales Any Price Reduction',\n",
    "            'Sum of Dollar Sales Feature and/or Display',\n",
    "            'Dollar Sales per Pt of Distribution',\n",
    "            'Units per Store Selling',\n",
    "            'Avg Weekly Units per Store Selling']\n",
    "target = 'Sum of Dollar Sales'\n",
    "\n",
    "df_all = df_num.dropna().astype(dtype='int32')\n",
    "df_all = df_all[features + [target]]\n",
    "df_train, df_test = train_test_split(df_all, test_size=0.15)\n",
    "\n",
    "X_train, y_train = df_train.drop('Sum of Dollar Sales', axis=1), df_train['Sum of Dollar Sales']\n",
    "X_test, y_test = df_test.drop('Sum of Dollar Sales', axis=1), df_test['Sum of Dollar Sales']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n",
    "                            max_features=1.0,\n",
    "                            min_samples_leaf=10, oob_score=True)\n",
    "rf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "                       max_depth=None, max_features=1.0, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_samples_leaf=10, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
    "                       oob_score=True, random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "I = importances(rf, X_test, y_test)\n",
    "plot_importances(I, width=12, vscale=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This importance plot from a RandomForestClassifier is one of the key\n",
    "### ways we'll understand which variables are the most important"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### With an array of all the variables, I could make an importance plot\n",
    "#### for everything but the categorical variables, but I would have to change\n",
    "#### the dependent variable each time. Remember that we dropped missing values\n",
    "#### instead of dropping columns, which we could do with a column that did not\n",
    "#### make an impression on the correlation plot or importance plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I = pd.DataFrame()\n",
    "\n",
    "I['Feature'] = X_train.columns\n",
    "I['Importance'] = rf.feature_importances_\n",
    "I = I.sort_values('Importance', ascending=False)\n",
    "I = I.set_index('Feature')\n",
    "viz = plot_importances(I, width=16, vscale=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Our categorical variables shouldn't be forgotten; just alter the threshold\n",
    "#### down from 250 if the axis titles start to look messy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include='object'):\n",
    "    if df[col].nunique() <= 30:\n",
    "        sns.countplot(y=col, data=df)\n",
    "        plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_reg = df[['Sum of Dollar Sales', 'Average Weekly ACV Distribution Feature Only',\n",
    "     'Average Weekly ACV Distribution Display Only']]\n",
    "\n",
    "viz = plot_corr_heatmap(df_reg, cmap='gist_heat_r', value_fontsize=10,\n",
    "                        figsize=(10, 10))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_reg['dollar_log'] = np.log(df_reg['Sum of Dollar Sales'])\n",
    "df_reg['feature_sq'] = np.sqrt(df_reg['Average Weekly ACV Distribution Feature Only'])\n",
    "df_reg['display_sq'] = np.sqrt(df_reg['Average Weekly ACV Distribution Display Only'])\n",
    "\n",
    "df_reg = df_reg.drop(['Sum of Dollar Sales',\n",
    "                      'Average Weekly ACV Distribution Feature Only',\n",
    "                      'Average Weekly ACV Distribution Display Only'], axis=1)\n",
    "\n",
    "viz = plot_corr_heatmap(df_reg, cmap='gist_heat_r', value_fontsize=10,\n",
    "                        figsize=(10, 10))\n",
    "viz.view()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_reg['retailer'] = df['Geography']\n",
    "df_reg['brand'] = df['Brand Value']\n",
    "\n",
    "df_reg.dropna()\n",
    "\n",
    "ch_ohe = ce.OneHotEncoder(cols=['retailer', 'brand'], use_cat_names=True)\n",
    "df_reg_ce = ch_ohe.fit_transform(df_reg)\n",
    "\n",
    "y_reg = df_reg_ce['dollar_log']\n",
    "X_reg = df_reg_ce.drop(['dollar_log'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X_reg, y_reg, test_size = 0.2, random_state = 13)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The regression formula is ln(y) = b_0 + b_1 * sqrt(feature) +\n",
    "#### b_2 * sqrt(display) + b_m * retailer + b_n*brand"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg_1 = reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = reg_1.predict(X_test)\n",
    "r_2 = r2_score(y_test, y_pred)\n",
    "print(r_2)\n",
    "\n",
    "lr = sm.OLS(y_reg, X_reg)\n",
    "lr_res = lr.fit()\n",
    "print(lr_res.summary())\n",
    "\n",
    "# constant was statistically insignificant"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "version": 1
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}